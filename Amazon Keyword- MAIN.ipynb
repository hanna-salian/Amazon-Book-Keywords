{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db146fc5",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "### Scrape the bestseller, Top Rated, etc lists\n",
    "1. Start with main books page, select each of the genre page to scrape. Open the 'Bestseller' and 'Top Rated' section.\n",
    "2. Open each book page and scrape the title, author, and description in a table for that genre. Delete duplicates\n",
    "3. Create SQL tables per genre and subgenre and insert all books into their tables.\n",
    "4. Sort most popular keywords and count the number of times they appear per subgenre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b0d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "import sys \n",
    "import csv\n",
    "\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import psycopg2.extras as extras\n",
    "from psycopg2 import Error\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import re\n",
    "from urllib.parse import urlencode\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869bcb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if \"twisted.internet.reactor\" in sys.modules:\n",
    "    del sys.modules[\"twisted.internet.reactor\"]\n",
    "\n",
    "    \n",
    "class KeywordSpider ( scrapy.Spider ):\n",
    "    name = \"amazon_keyword\"\n",
    "    asin = ''\n",
    "    genre = ''\n",
    "    subgenre = ''\n",
    "    product_url = ''\n",
    "    \n",
    "    def start_requests( self ):\n",
    "        \n",
    "\n",
    "        HEADERS = ({'User-Agent':\n",
    "            'a-keyword-project (https://amazon.com)'\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "            'Accept-Language': 'en-US, en;q=0.5',\n",
    "            'Redirect_enabled': 'true'})\n",
    "        \n",
    "        url = 'https://www.amazon.com/books-used-books-textbooks/b/?ie=UTF8&node=283155'\n",
    "        yield scrapy.Request(url = get_url(url), headers = HEADERS, callback = self.parse_books_page)\n",
    "\n",
    "       \n",
    "    def parse_books_page( self, response ):\n",
    "        genres_start = response.xpath('//li[@class=\"a-spacing-micro apb-browse-refinements-indent-2\"]')\n",
    "        genre_links = genres_start.xpath('.//a/@href')\n",
    "        links_to_follow = genre_links.extract()\n",
    "        \n",
    "        #Collect the genre titles\n",
    "        genre_title = genres_start.xpath('.//span[@dir=\"auto\"]//text()').extract()\n",
    "        clean_genres_all = [ re.sub('\\W+','_', element) for element in genre_title ]\n",
    "        print(clean_genres_all)\n",
    "\n",
    "                \n",
    "        for i in range(len(links_to_follow)):\n",
    "            links_to_follow[i] = \"https://www.amazon.com\" + links_to_follow[i]\n",
    "        \n",
    "        links_and_genres = list(zip(clean_genres_all, links_to_follow))\n",
    "        \n",
    "        global genre\n",
    "        \n",
    "        for genre, url in links_and_genres:\n",
    "            print(genre)\n",
    "            delay()\n",
    "            yield response.follow(url = get_url(url), callback = self.parse_subgenres, meta={'genre': genre})\n",
    "\n",
    "            \n",
    "    def parse_subgenres( self, response ):\n",
    "        global genre\n",
    "\n",
    "        subgenres_start = response.xpath('//li[@class=\"a-spacing-micro apb-browse-refinements-indent-2\"]')\n",
    "        subgenre_links = subgenres_start.xpath('.//a/@href')\n",
    "        links_to_follow = subgenre_links.extract()\n",
    "        \n",
    "        #Collect the genre titles\n",
    "        subgenre_title = subgenres_start.xpath('.//span[@dir=\"auto\"]//text()').extract()\n",
    "        clean_subgenre = [ re.sub('\\W+','_', element) for element in subgenre_title ]\n",
    "        print(clean_subgenre)\n",
    "        \n",
    "                \n",
    "        for i in range(len(links_to_follow)):\n",
    "            links_to_follow[i] = \"https://www.amazon.com\" + links_to_follow[i]\n",
    "        \n",
    "        links_and_genres = list(zip(clean_subgenre, links_to_follow))\n",
    "        global subgenre\n",
    "        \n",
    "        for subgenre, url in links_and_genres:\n",
    "            print(subgenre)\n",
    "            delay()\n",
    "            yield response.follow(url = get_url(url), callback = self.parse_genres, meta={'genre': genre, 'subgenre' : subgenre})\n",
    "            \n",
    "        \n",
    "    def parse_genres (self, response):\n",
    " \n",
    "        genre = response.meta['genre']\n",
    "        subgenre = response.meta['subgenre']\n",
    "        \n",
    "        top_rated_block = response.xpath('//div[@class=\"a-section octopus-pc-card-title\"]')\n",
    "        top_rated_links = top_rated_block.xpath('.//a/@href')\n",
    "        links_to_follow = top_rated_links.extract()\n",
    "        \n",
    "        print(\"in parse_genres: \", genre)\n",
    "\n",
    "        for i in range(len(links_to_follow)):\n",
    "            links_to_follow[i] = \"https://www.amazon.com\" + links_to_follow[i]\n",
    "       \n",
    "\n",
    "        for url in links_to_follow:\n",
    "            delay()\n",
    "            yield response.follow(url = get_url(url), callback = self.parse_keyword_response, meta={'genre': genre, 'subgenre' : subgenre})\n",
    "            \n",
    "    \n",
    "    def parse_keyword_response(self, response):\n",
    "        \n",
    "        global asin\n",
    "        global product_url\n",
    "        \n",
    "        \n",
    "        genre = response.meta['genre']\n",
    "        subgenre = response.meta['subgenre']\n",
    "        \n",
    "        df_csv_file = pd.read_csv('amazon_keywords.csv')\n",
    "        asin_column = df_csv_file['asin'] \n",
    "        my_set = set(asin_column)\n",
    "        \n",
    "        products = response.xpath('//*[@data-asin]')\n",
    "\n",
    "        for product in products:\n",
    "            asin = product.xpath('@data-asin').extract_first()\n",
    "            print(asin)\n",
    "            \n",
    "            if asin in my_set: \n",
    "                pass\n",
    "                print(\"PASSED\")\n",
    "            else:\n",
    "                product_url = f\"https://www.amazon.com/dp/{asin}\"\n",
    "                print(product_url)\n",
    "                yield response.follow(url= get_url(product_url), callback=self.parse_all, meta={'asin': asin, 'genre': genre, 'subgenre' : subgenre, 'product_url': product_url})\n",
    "\n",
    "        next_page = response.xpath('//li[@class=\"a-last\"]/a/@href').extract_first()\n",
    "        if next_page:\n",
    "            print(\"Next Page\")\n",
    "            url = urljoin(\"https://www.amazon.com\",next_page)\n",
    "            yield scrapy.Request(url = get_url(product_url), callback = self.parse_keyword_response, meta={'genre': genre, 'subgenre' : subgenre})\n",
    "\n",
    "            \n",
    "    \n",
    "    def parse_all (self, response):\n",
    "\n",
    "        asin = response.meta['asin']\n",
    "        genre = response.meta['genre']\n",
    "        subgenre = response.meta['subgenre']\n",
    "        product_url = response.meta['product_url']\n",
    "        \n",
    "        \n",
    "        print(\"in parse_all: \", subgenre)\n",
    "        # Create a SelectorList of the course titles text\n",
    "        crs_title = response.xpath('//span[@id=\"productTitle\"]//text()').get() \n",
    "        print(crs_title)\n",
    "\n",
    "        # Create a SelectorList of course descriptions text\n",
    "        crs_descr = response.xpath('//*[@id=\"bookDescription_feature_div\"]//text()').extract()\n",
    "                \n",
    "        #Clean up repeat blank elements in list\n",
    "        crs_descr = [i for a,i in enumerate(crs_descr) if i!=' ' ]\n",
    "        crs_descr = [i for a,i in enumerate(crs_descr) if i!='  ' ]\n",
    "        crs_descr = [i for a,i in enumerate(crs_descr) if i!='                                 ' ]\n",
    "        crs_descr = [i for a,i in enumerate(crs_descr) if i!='\\n                                     ' ]\n",
    "        crs_descr = [i for a,i in enumerate(crs_descr) if i!='Read more' ]\n",
    "        crs_descr = [i for a,i in enumerate(crs_descr) if i!='Read less' ]\n",
    "        crs_descr = [i for a,i in enumerate(crs_descr) if i!='xa0' ]\n",
    "        \n",
    "        \n",
    "        all_asin.append(asin)\n",
    "        all_genres.append(genre)\n",
    "        all_subgenres.append(subgenre)\n",
    "        all_product_url.append(product_url)\n",
    "        all_titles.append(crs_title)\n",
    "        all_descr.append(crs_descr)\n",
    "        \n",
    "\n",
    "\n",
    "API = 'API nonsense: letters numbers and such'\n",
    "def get_url(url):\n",
    "    payload = {'api_key': API, 'url': url}\n",
    "    proxy_url = 'http://api.scraperapi.com/?' + urlencode(payload)\n",
    "    return proxy_url\n",
    "\n",
    "\n",
    "\n",
    "def delay():\n",
    "    time.sleep(random.randint(3, 10))\n",
    "\n",
    "clean_genres_all = list()\n",
    "all_asin = []\n",
    "all_genres = []\n",
    "all_subgenres = []\n",
    "all_product_url = []\n",
    "all_titles = []\n",
    "all_descr = []\n",
    "\n",
    "\n",
    "process = CrawlerProcess()\n",
    "process.crawl(KeywordSpider)\n",
    "process.start()\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'asin': all_asin,\n",
    "    'genre': all_genres,\n",
    "    'subgenre': all_subgenres,\n",
    "    'product_url': all_product_url,\n",
    "    'title': all_titles, \n",
    "    'description': all_descr\n",
    "})\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53baadfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "df.to_csv('amazon_keywords.csv', mode='a', index=True, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90654641",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_csv_file = pd.read_csv('amazon_keywords.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e86b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2ce288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and download packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from collections import Counter\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Tokenize the text\n",
    "df_token = df['description'].astype(str)\n",
    "token_two = [tokenizer.tokenize(i)for i in df_token]\n",
    "\n",
    "\n",
    "# Create a list called words containing all tokens transformed to lowercase\n",
    "words = [[word.lower() for word in token] for token in token_two]\n",
    "\n",
    "# Get the English stop words from nltk\n",
    "all_stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Add Amazon books specific stopwords\n",
    "keyword_stop_list = ['bestseller', 'bestselling', '1', 'bestseller', 'author', 'amazon', 'national', 'international',\n",
    "                     'new york times', 'audible', 'kindle', 'must', 'never', 'novel', 'book', 'also', 'brandon', 'every',\n",
    "                     'would', 'get', 'even', 'new', 'york', 'times', 'us', 'kickstarter', 'well', 'xa0', 'f', \n",
    "                     't', 'ck', 'n', 'preorder', 'x', 'b', 'c', 'd', 'e', 'h', 'j', 'k', 'l', 'm', 'o', 'p', 'q', \n",
    "                     'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'st']\n",
    "all_stopwords.extend(keyword_stop_list)\n",
    "\n",
    "# Remove all stopwords\n",
    "output_array=[]\n",
    "for sentence in words:\n",
    "    temp_list=[]\n",
    "    for word in sentence:\n",
    "        if word not in all_stopwords:\n",
    "            temp_list.append(word)\n",
    "    output_array.append(temp_list)\n",
    "        \n",
    "#print(output_array)\n",
    "\n",
    "# Make a list of all the words in the genre descriptions\n",
    "concat_list = [j for i in output_array for j in i]\n",
    "\n",
    "# Make a concatinated list of all the keywords in each description\n",
    "concat_list_group = [\" \".join(i) for i in output_array]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88422c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: 'postgresql://username:password@localhost:5432/your_database'\n",
    "engine = create_engine('postgresql://username:password@localhost:5432/keyword_database')\n",
    "\n",
    "start_time = time.time() # get start time before insert\n",
    "\n",
    "df_csv_file.to_sql(\n",
    "    name=\"all_genres_with_subgenres\", # table name\n",
    "    con=engine,  # engine\n",
    "    if_exists=\"append\", #  If the table already exists, append\n",
    "    index=True # no index\n",
    ")\n",
    "\n",
    "end_time = time.time() # get end time after insert\n",
    "total_time = end_time - start_time # calculate the time\n",
    "print(f\"Insert time: {total_time} seconds\") # print time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a124cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"dbname=keyword_database user=username password=password\")\n",
    "\n",
    "# Open a cursor to perform database operations\n",
    "cur = conn.cursor()\n",
    "\n",
    "def delete_duplicates():\n",
    "    cur.execute(\"\"\"DELETE FROM all_genres_with_subgenres\n",
    "                    WHERE (asin) IN (\n",
    "                        SELECT asin\n",
    "                        FROM all_genres_with_subgenres\n",
    "                        GROUP BY  asin\n",
    "                        HAVING COUNT(*) > 1\n",
    "                    );\n",
    "                \"\"\")\n",
    "    conn.commit()\n",
    "\n",
    "def insert_keywords( ):\n",
    "    \n",
    "    row_counter = 0\n",
    "    # Looping through each item in the list\n",
    "    for descr_list in concat_list_group:\n",
    "        list_counter = 0\n",
    "        print(descr_list)\n",
    "        count_indiv_descr = Counter(descr_list.split())\n",
    "        top_five = count_indiv_descr.most_common(5)\n",
    "        print(top_five)\n",
    "        \n",
    "        cur.execute(\"UPDATE all_genres_with_subgenres SET all_keywords = '{0}' WHERE index = %s\".format(descr_list), [row_counter])\n",
    "        conn.commit()\n",
    "        for indiv in top_five:\n",
    "            print(indiv)\n",
    "            top_numerated = ['first', 'second', 'third', 'fourth', 'fifth']\n",
    "            print(top_numerated[list_counter])\n",
    "            cur.execute(\"UPDATE all_genres_with_subgenres SET \" + top_numerated[list_counter] + \"_top_keyword = {0} WHERE index = %s\".format(indiv), [row_counter])\n",
    "            list_counter += 1\n",
    "            conn.commit()\n",
    "        row_counter += 1\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "cur.execute(\"\"\"ALTER TABLE all_genres_with_subgenres\n",
    "                ADD COLUMN IF NOT EXISTS top_keywords_for_genre VARCHAR(255),\n",
    "                ADD COLUMN IF NOT EXISTS all_keywords VARCHAR(10000),\n",
    "                ADD COLUMN IF NOT EXISTS first_top_keyword VARCHAR(255),\n",
    "                ADD COLUMN IF NOT EXISTS second_top_keyword VARCHAR(255),\n",
    "                ADD COLUMN IF NOT EXISTS third_top_keyword VARCHAR(255),\n",
    "                ADD COLUMN IF NOT EXISTS fourth_top_keyword VARCHAR(255),\n",
    "                ADD COLUMN IF NOT EXISTS fifth_top_keyword VARCHAR(255)\n",
    "                \"\"\")\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "\n",
    "insert_keywords()\n",
    "delete_duplicates()\n",
    "\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d776f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_genres_all = ['Arts_Photography', 'Biographies_Memoirs', 'Business_Money', 'Calendars', 'Children_s_Books', 'Christian_Books_Bibles', 'Comics_Graphic_Novels', 'Computers_Technology', 'Cookbooks_Food_Wine', 'Crafts_Hobbies_Home', 'Education_Teaching', 'Engineering_Transportation', 'Health_Fitness_Dieting', 'History', 'Humor_Entertainment', 'Law', 'LGBTQ_Books', 'Literature_Fiction', 'Medical_Books', 'Mystery_Thriller_Suspense', 'Parenting_Relationships', 'Politics_Social_Sciences', 'Reference', 'Religion_Spirituality', 'Romance', 'Science_Fiction_Fantasy', 'Science_Math', 'Self_Help', 'Sports_Outdoors', 'Teen_Young_Adult', 'Test_Preparation', 'Travel']\n",
    "print(clean_genres_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a67bfee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Connect to your postgres DB\n",
    "conn = psycopg2.connect(\"dbname=keyword_database user=username password=password\")\n",
    "\n",
    "# Open a cursor to perform database operations\n",
    "cur = conn.cursor()\n",
    "\n",
    "def create_genre_tables(genre):\n",
    "    cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS \"\"\" + genre + \"\"\"(\n",
    "               id SERIAL PRIMARY KEY,\n",
    "               genre VARCHAR(255),\n",
    "               subgenre VARCHAR(255),\n",
    "               title VARCHAR(255),\n",
    "               description VARCHAR(10000),\n",
    "               all_keywords VARCHAR(10000),\n",
    "               first_top_keyword VARCHAR(255),\n",
    "               second_top_keyword VARCHAR(255),\n",
    "               third_top_keyword VARCHAR(255),\n",
    "               fourth_top_keyword VARCHAR(255),\n",
    "               fifth_top_keyword VARCHAR(255),\n",
    "               asin VARCHAR(255),\n",
    "               product_url VARCHAR(255)\n",
    "               )\n",
    "               \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    \n",
    "    \n",
    "def create_subgenre_tables(genre, subgenre):\n",
    "    cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS \"\"\" + subgenre + \"\"\"(\n",
    "               id SERIAL PRIMARY KEY,\n",
    "               genre VARCHAR(255),\n",
    "               subgenre VARCHAR(255),\n",
    "               title VARCHAR(255),\n",
    "               description VARCHAR(10000),\n",
    "               all_keywords VARCHAR(10000),\n",
    "               first_top_keyword VARCHAR(255),\n",
    "               second_top_keyword VARCHAR(255),\n",
    "               third_top_keyword VARCHAR(255),\n",
    "               fourth_top_keyword VARCHAR(255),\n",
    "               fifth_top_keyword VARCHAR(255),\n",
    "               asin VARCHAR(255),\n",
    "               product_url VARCHAR(255),\n",
    "               FOREIGN KEY (subgenre)\n",
    "                  REFERENCES \"\"\" + genre + \"\"\"(subgenre)\n",
    "                  ON DELETE CASCADE)\n",
    "               \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    \n",
    "    \n",
    "for x in clean_genres_all:\n",
    "    print(x)\n",
    "    create_genre_tables(x)\n",
    "    for y in <something>:\n",
    "        print(y)\n",
    "        create_subgenre_tables(x, y)\n",
    "    \n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7c669e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"dbname=keyword_database user=username password=password\")\n",
    "\n",
    "# Open a cursor to perform database operations\n",
    "cur = conn.cursor()\n",
    "\n",
    "    \n",
    "def insert_genres (genre):\n",
    "    cur.execute(\"\"\"INSERT INTO \"\"\" + genre + \"\"\" ( genre, subgenre, asin, product_url, title, description, \n",
    "                        all_keywords, first_top_keyword, second_top_keyword, \n",
    "                        third_top_keyword, fourth_top_keyword, fifth_top_keyword)\n",
    "                SELECT DISTINCT genre, subgenre, asin, product_url, title, description, all_keywords, \n",
    "                        first_top_keyword, second_top_keyword, \n",
    "                        third_top_keyword, fourth_top_keyword, fifth_top_keyword\n",
    "                FROM all_genres_with_subgenres\n",
    "                WHERE genre = '\"\"\" +  genre + \"\"\"'\n",
    "                \"\"\")\n",
    "    print (genre)\n",
    "    conn.commit()\n",
    "\n",
    "    \n",
    "def insert_subgenres (genre, subgenre):\n",
    "    cur.execute(\"\"\"INSERT INTO \"\"\" + subgenre + \"\"\" ( genre, subgenre, asin, product_url, title, description, \n",
    "                        all_keywords, first_top_keyword, second_top_keyword, \n",
    "                        third_top_keyword, fourth_top_keyword, fifth_top_keyword)\n",
    "                SELECT DISTINCT genre, subgenre, asin, product_url, title, description, all_keywords, \n",
    "                        first_top_keyword, second_top_keyword, \n",
    "                        third_top_keyword, fourth_top_keyword, fifth_top_keyword\n",
    "                FROM \"\"\" + genre + \"\"\"\n",
    "                WHERE subgenre = '\"\"\" +  subgenre + \"\"\"'\n",
    "                \"\"\")\n",
    "    print (genre)\n",
    "    conn.commit()\n",
    "def find_top_keys (genre):\n",
    "    \n",
    "    query = \"SELECT all_keywords FROM {0} ;\".format(genre)\n",
    "\n",
    "    cur.execute(query)\n",
    "    \n",
    "    all_keys = cur.fetchall()\n",
    "    word_two = [x for word in all_keys for x in word]\n",
    "    token_two = [tokenizer.tokenize(i) for i in word_two]\n",
    "\n",
    "    output_array=[]\n",
    "    for sentence in word_two:\n",
    "        temp_list=[]\n",
    "        for word in sentence:\n",
    "            temp_list.append(word)\n",
    "        output_array.append(temp_list)\n",
    "\n",
    "\n",
    "    concat_key_list = [j for i in token_two for j in i]\n",
    "\n",
    "    \n",
    "    # Initialize a Counter object from our processed list of words\n",
    "    count_all = Counter(concat_key_list)\n",
    "    print(count_all)\n",
    "    \n",
    "    # Store ten most common words and their counts as top_ten\n",
    "    top_ten = count_all.most_common(10)\n",
    "\n",
    "    # Print the top ten words and their counts\n",
    "    print(\"Added the top ten in\", genre)\n",
    "    print(top_ten)\n",
    "    cur.execute(\"ALTER TABLE \"+ genre +\" ADD COLUMN IF NOT EXISTS top_keywords_for_genre VARCHAR(255)\")\n",
    "\n",
    "    all_words = []\n",
    "    for common_words, *rest in top_ten:\n",
    "        all_words.append(common_words)\n",
    "        \n",
    "    concat_keys = ', '.join(all_words)\n",
    "\n",
    "    cur.execute(\"UPDATE \" + genre + \" SET top_keywords_for_genre = '{0}' \".format(concat_keys) )\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "for x in clean_genres_all:\n",
    "    insert_genres(x)\n",
    "    find_top_keys(x)\n",
    "   \n",
    "    for y in <something>:\n",
    "        insert_subgenres(x,y)\n",
    "        find_top_keys(y)\n",
    "\n",
    "    \n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1138e9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
